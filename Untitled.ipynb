{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37476931",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/admin/t5-experiments-from-yura/venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from pymongo import MongoClient\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoConfig, AutoTokenizer, HfArgumentParser, T5ForConditionalGeneration, T5Config, PretrainedConfig\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62a84b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvalBatch:\n",
    "    def __init__(self, items, tokenizer):\n",
    "#         print(items)\n",
    "        self.inputs = [item['input'] for item in items]\n",
    "        self.target_text = [item[\"outputs\"] for item in items]\n",
    "        self.inputs_tokenized = tokenizer(self.inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    # custom memory pinning method on custom type\n",
    "    def pin_memory(self):\n",
    "        self.inputs_tokenized.input_ids = self.inputs_tokenized.input_ids.pin_memory()\n",
    "        self.inputs_tokenized.attention_mask = self.inputs_tokenized.attention_mask.pin_memory()\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d19b1e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KGLMDataset(Dataset):\n",
    "    def __init__(self, port, db, collection):\n",
    "        self.client = MongoClient('localhost', port)\n",
    "        self.db_name = db\n",
    "        self.collection_name = collection\n",
    "        self.collection = self.client[db][collection]\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "        self.tokenizer.add_tokens(['[SEP]'], special_tokens=True)\n",
    "        self.length = self.client[self.db_name].command(\"collstats\", self.collection_name)['count']\n",
    "\n",
    "    def  __getitem__(self, idx):\n",
    "        item = {}\n",
    "        doc = self.collection.find_one({'_id': str(idx)})\n",
    "        item[\"input\"] = doc['verbalization']\n",
    "        item[\"outputs\"] = doc['target']\n",
    "        return item\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def _collate_eval(self, batch):\n",
    "        encode_plus_kwargs = {'truncation': True, 'padding': 'longest', 'pad_to_multiple_of': 1}\n",
    "\n",
    "        \n",
    "        inputs = [b['input'] for b in batch]\n",
    "        inputs_tokenized = self.tokenizer.batch_encode_plus(list(inputs), max_length=512, return_tensors='pt',\n",
    "                                                   **encode_plus_kwargs)\n",
    "        \n",
    "        target_text = [b[\"outputs\"] for b in batch]\n",
    "\n",
    "        return inputs_tokenized.input_ids, inputs_tokenized.attention_mask, target_text\n",
    "\n",
    "    def _collate_eval_with_input_strings(self, items):\n",
    "        return EvalBatch(items, self.tokenizer)\n",
    "        # inputs = [item[0] for item in items]\n",
    "        # target_text = [item[1] for item in items]\n",
    "        # inputs_tokenized = self.tokenizer(inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        # return inputs_tokenized.input_ids, inputs_tokenized.attention_mask, target_text, inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8e51373",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/admin/t5-experiments-from-yura/venv/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py:156: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dataset = KGLMDataset(27017, 'KGLM', 'test')\n",
    "# data_loader = DataLoader(\n",
    "#     dataset,\n",
    "#     batch_size=1,\n",
    "#     shuffle=False,\n",
    "#     num_workers=1,\n",
    "#     collate_fn=dataset._collate_eval\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fe009ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "PytorchStreamReader failed reading file data/123724256: invalid header or archive is corrupted",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m model_cfg \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt5-small\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m T5ForConditionalGeneration(config\u001b[38;5;241m=\u001b[39mmodel_cfg)\n\u001b[0;32m----> 9\u001b[0m cpt \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_cpt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(cpt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/data/home/admin/t5-experiments-from-yura/venv/lib/python3.8/site-packages/torch/serialization.py:712\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    710\u001b[0m             opened_file\u001b[38;5;241m.\u001b[39mseek(orig_position)\n\u001b[1;32m    711\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mload(opened_file)\n\u001b[0;32m--> 712\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n",
      "File \u001b[0;32m/data/home/admin/t5-experiments-from-yura/venv/lib/python3.8/site-packages/torch/serialization.py:1049\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1047\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1048\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1049\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1051\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/data/home/admin/t5-experiments-from-yura/venv/lib/python3.8/site-packages/torch/serialization.py:1019\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m loaded_storages:\n\u001b[1;32m   1018\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1019\u001b[0m     \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loaded_storages[key]\n",
      "File \u001b[0;32m/data/home/admin/t5-experiments-from-yura/venv/lib/python3.8/site-packages/torch/serialization.py:997\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_tensor\u001b[39m(dtype, numel, key, location):\n\u001b[1;32m    995\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 997\u001b[0m     storage \u001b[38;5;241m=\u001b[39m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_storage_from_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_UntypedStorage\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstorage()\u001b[38;5;241m.\u001b[39m_untyped()\n\u001b[1;32m    998\u001b[0m     \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;66;03m# stop wrapping with _TypedStorage\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m     loaded_storages[key] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39m_TypedStorage(\n\u001b[1;32m   1001\u001b[0m         wrap_storage\u001b[38;5;241m=\u001b[39mrestore_location(storage, location),\n\u001b[1;32m   1002\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: PytorchStreamReader failed reading file data/123724256: invalid header or archive is corrupted"
     ]
    }
   ],
   "source": [
    "path = 'lr5e-05_constant_with_warmup_adamw_wd1e-03_512-512_bs64_iters4000000/run_1/'\n",
    "\n",
    "model_cpt = os.path.join('.', 'model_best-3.pth')\n",
    "config_path = os.path.join('t5configs', 't5-small.json')\n",
    "\n",
    "model_cfg = AutoConfig.from_pretrained('t5-small')\n",
    "model = T5ForConditionalGeneration(config=model_cfg)\n",
    "\n",
    "cpt = torch.load(model_cpt, map_location='cpu')\n",
    "model.load_state_dict(cpt['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b086b6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self, length_penalty=1.0, max_output_length=20, length_normalization=0,\n",
    "                 batch_size=1, beam_size=1, save_file='neigh_model', num_predictions=200):\n",
    "        self.batch_size = batch_size\n",
    "        self.beam_size = beam_size\n",
    "        self.save_file = save_file\n",
    "        self.num_predictions = num_predictions\n",
    "        self.length_penalty = length_penalty\n",
    "        self.max_output_length = max_output_length\n",
    "        self.length_normalization = length_normalization\n",
    "args = Args(save_file='baseline_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e07227d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getScores(ids, scores, pad_token_id, length_normalization = 0):\n",
    "    # ids is list of tokenized strings\n",
    "    # scores is a list of tensors. each tensor contains score of each token in vocab\n",
    "    # conditioned on ids till that point\n",
    "    # stack scores\n",
    "    scores = torch.stack(scores, dim=1)\n",
    "    \n",
    "    # after stacking, shape is (batch_size*num_return_sequences, num tokens in sequence, vocab size)\n",
    "    # get probs\n",
    "    log_probs = torch.log_softmax(scores, dim=2)\n",
    "    # remove start token\n",
    "    ids = ids[:,1:]\n",
    "    # gather needed probs\n",
    "    x = ids.unsqueeze(-1).expand(log_probs.shape)\n",
    "    needed_logits = torch.gather(log_probs, 2, x)\n",
    "    final_logits = needed_logits[:, :, 0]\n",
    "    padded_mask = (ids == pad_token_id)\n",
    "    final_logits[padded_mask] = 0\n",
    "    final_scores = final_logits.sum(dim=-1)\n",
    "    if length_normalization == 1:\n",
    "        sequence_lengths = torch.sum(~padded_mask, dim=1)\n",
    "        final_scores = final_scores/sequence_lengths\n",
    "\n",
    "    return final_scores.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1f7c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouper(arr, n):\n",
    "    \"grouper(3, 'ABCDEFG', 'x') --> ABC DEF Gxx\"\n",
    "    total = len(arr)\n",
    "    if total % n != 0:\n",
    "        raise ValueError('Cannot divide %d by %d' % (total, n))\n",
    "    out = []\n",
    "    for i in range(int(total/n)):\n",
    "        start_id = i * n\n",
    "        out.append(arr[start_id:start_id+n])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02350848",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, dataset, args):\n",
    "    num_workers = 1\n",
    "    batch_size = args.batch_size\n",
    "    # batch_size = 200\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    print('Using model.generate')\n",
    "\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers,\n",
    "        collate_fn=dataset._collate_eval_with_input_strings, pin_memory=True)\n",
    "    \n",
    "    loader = tqdm(data_loader, total=len(data_loader), unit=\"batches\")\n",
    "    i = 0\n",
    "    targets = []\n",
    "    predictions = []\n",
    "    prediction_scores = []\n",
    "    model_inputs = []\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for steps, batch in enumerate(loader):\n",
    "            \n",
    "            input_ids, attention_mask, target_text, input_text = batch.inputs_tokenized.input_ids, \\\n",
    "                batch.inputs_tokenized.attention_mask, batch.target_text, batch.inputs\n",
    "                \n",
    "            outputs = model.generate(input_ids = input_ids.cuda(), attention_mask=attention_mask.cuda(),\n",
    "                        temperature=1.0, #TODO: make this argument?\n",
    "                        do_sample=True,\n",
    "                        num_return_sequences = args.num_predictions,\n",
    "                        num_beams = args.beam_size,\n",
    "                        eos_token_id = dataset.tokenizer.eos_token_id,\n",
    "                        pad_token_id = dataset.tokenizer.pad_token_id,\n",
    "                        output_scores = True,\n",
    "                        return_dict_in_generate=True,\n",
    "                        length_penalty = args.length_penalty,\n",
    "                        max_length=args.max_output_length,\n",
    "                        # top_p=0.95,\n",
    "                        # top_k=250,\n",
    "                        #  prefix_allowed_tokens_fn=prefixFn,\n",
    "                        )\n",
    "            sequences = outputs.sequences\n",
    "            \n",
    "            if args.beam_size > 1:\n",
    "                final_scores = outputs.sequences_scores\n",
    "                if args.length_normalization == 1:\n",
    "                    # get sequence lengths. see getScores for how this works\n",
    "                    sequence_lengths = torch.sum((sequences[:,1:] != dataset.tokenizer.pad_token_id), dim=1)\n",
    "                    final_scores = final_scores/sequence_lengths\n",
    "                final_scores = final_scores.cpu()\n",
    "\n",
    "            else:\n",
    "                scores = outputs.scores\n",
    "                final_scores = getScores(sequences, scores, dataset.tokenizer.pad_token_id,\n",
    "                                         length_normalization = args.length_normalization)\n",
    "            \n",
    "            predicted_text = dataset.tokenizer.batch_decode(sequences, skip_special_tokens=True)\n",
    "            \n",
    "            if len(predicted_text) == len(input_text):\n",
    "                final_scores = final_scores.tolist()\n",
    "            else:\n",
    "                predicted_text = grouper(predicted_text, args.num_predictions) # grouping only needed if multiple predictions\n",
    "                final_scores = grouper(final_scores, args.num_predictions)\n",
    "                \n",
    "            targets.extend(target_text)\n",
    "            model_inputs.extend(input_text)\n",
    "            predictions.extend(predicted_text)\n",
    "            prediction_scores.extend(final_scores)\n",
    "#             print(targets)\n",
    "#             print(predictions)\n",
    "#             print(prediction_scores)\n",
    "\n",
    "    correct = 0     \n",
    "    num_not_in_entities = 0\n",
    "    for p, t in zip(predictions, targets):\n",
    "\n",
    "        if t in p:\n",
    "            correct += 1\n",
    "\n",
    "    # print(num_not_in_entities/len(predictions), 'predictions were not entities')\n",
    "    data_to_save = {'prediction_strings': predictions, \n",
    "                    'scores': prediction_scores,\n",
    "                    'target_strings': targets,\n",
    "                    'input_strings': model_inputs}\n",
    "    fname = 'scores/' + args.save_file + '.pickle'\n",
    "    pickle.dump(data_to_save, open(fname, 'wb'))\n",
    "    accuracy = correct/len(targets)\n",
    "    return accuracy    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb77ab66",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval(model, dataset, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8f92f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
