{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed5865b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/admin/t5-experiments-from-yura/venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from typing import Dict\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from pymongo import MongoClient\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoConfig, AutoTokenizer, HfArgumentParser, T5ForConditionalGeneration, T5Config, PretrainedConfig\n",
    "import os\n",
    "from transformers import (\n",
    "    LogitsProcessorList,\n",
    "    MinLengthLogitsProcessor,\n",
    "    BeamSearchScorer,\n",
    ")\n",
    "from typing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a4124d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = []\n",
    "with open(\"wd5m_aliases_entities_v3.txt\", 'r') as f:\n",
    "    for line in f:\n",
    "        entities.append(line.split('\\t')[1][:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e76e6bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['human',\n",
       "  'United States of America',\n",
       "  'taxon',\n",
       "  'species',\n",
       "  'United Kingdom',\n",
       "  'English',\n",
       "  'association football',\n",
       "  'politician',\n",
       "  'association football player',\n",
       "  'UTC+01:00'],\n",
       " 4818679)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities[:10], len(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d948417",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = entities[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8180ea52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KGLMDataset(Dataset):\n",
    "    def __init__(self, port, db, collection):\n",
    "        self.client = MongoClient('localhost', port)\n",
    "        self.db_name = db\n",
    "        self.collection_name = collection\n",
    "        self.collection = self.client[db][collection]\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "        self.tokenizer.add_tokens(['[SEP]'], special_tokens=True)\n",
    "        self.length = self.client[self.db_name].command(\"collstats\", self.collection_name)['count']\n",
    "\n",
    "    def  __getitem__(self, idx):\n",
    "        item = {}\n",
    "        doc = self.collection.find_one({'_id': str(idx)})\n",
    "        item[\"input\"] = doc['verbalization']\n",
    "        item[\"outputs\"] = doc['target']\n",
    "        return item\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def _collate_eval(self, batch):\n",
    "        encode_plus_kwargs = {'truncation': True, 'padding': 'longest', 'pad_to_multiple_of': 1}\n",
    "\n",
    "        \n",
    "        inputs = [b['input'] for b in batch]\n",
    "        inputs_tokenized = self.tokenizer.batch_encode_plus(list(inputs), max_length=512, return_tensors='pt',\n",
    "                                                   **encode_plus_kwargs)\n",
    "        \n",
    "        target_text = [b[\"outputs\"] for b in batch]\n",
    "\n",
    "        return inputs_tokenized.input_ids, inputs_tokenized.attention_mask, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e046c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/admin/t5-experiments-from-yura/venv/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py:156: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dataset = KGLMDataset(27017, 'KGLM', 'test')\n",
    "data_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "    collate_fn=dataset._collate_eval\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "641aa50d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'lr5e-05_constant_with_warmup_adamw_wd1e-03_512-512_bs64_iters4000000/run_1/'\n",
    "\n",
    "model_cpt = os.path.join(path, 'model_best.pth')\n",
    "config_path = os.path.join(path, 'config.json')\n",
    "\n",
    "model_cfg = AutoConfig.from_pretrained('t5-small')\n",
    "model = T5ForConditionalGeneration(config=model_cfg)\n",
    "\n",
    "cpt = torch.load(model_cpt, map_location='cpu')\n",
    "model.load_state_dict(cpt['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac5c8b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self, batch_size=1, chunk_size=50, num_workers=2, device='cuda'):\n",
    "        self.batch_size = batch_size\n",
    "        self.beam_size = 11\n",
    "        self.num_predictions = 10\n",
    "        self.length_penalty = 0.3\n",
    "        self.num_workers = num_workers\n",
    "        self.device = device\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8e3ab08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_multi_old(model, dataset, args):\n",
    "    num_workers = 1\n",
    "    batch_size = args.batch_size\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    \n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers,\n",
    "                            collate_fn=dataset._collate_eval)\n",
    "    loader = tqdm(data_loader, total=len(data_loader), unit=\"batches\")\n",
    "    i = 0\n",
    "    beam_size = args.beam_size\n",
    "    num_predictions = args.num_predictions\n",
    "    length_penalty = args.length_penalty\n",
    "    correct = 0\n",
    "    print('Beams: %d, Predictions: %d, Length Penalty: %f' % (beam_size, num_predictions, length_penalty))\n",
    "    \n",
    "    for steps, batch in enumerate(loader):\n",
    "        \n",
    "        encoder_input_ids, attention_mask, target_text = batch\n",
    "        encoder_input_ids = encoder_input_ids.cuda()\n",
    "        attention_mask = attention_mask.cuda()\n",
    "        input_ids = torch.ones((len(encoder_input_ids) * beam_size, 1), device=model.device, dtype=torch.long)\n",
    "        \n",
    "        input_ids = input_ids * model.config.decoder_start_token_id\n",
    "\n",
    "        model_kwargs = {\n",
    "            \"encoder_outputs\": model.get_encoder()(encoder_input_ids.repeat_interleave(beam_size, dim=0), return_dict=True)\n",
    "        }\n",
    "\n",
    "        beam_scorer = BeamSearchScorer(\n",
    "            batch_size=len(encoder_input_ids),\n",
    "            num_beams=beam_size,\n",
    "            device=model.device,\n",
    "            num_beam_hyps_to_keep=num_predictions,\n",
    "            length_penalty = length_penalty\n",
    "        )\n",
    "        logits_processor = LogitsProcessorList([])\n",
    "        outputs = model.beam_search(input_ids, beam_scorer, logits_processor=logits_processor, **model_kwargs, max_length=64)\n",
    "        # outputs = model.generate(input_ids = encoder_input_ids)\n",
    "        # target_text = dataset.tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        predicted_text = dataset.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        \n",
    "        input_text = dataset.tokenizer.batch_decode(encoder_input_ids, skip_special_tokens=True)\n",
    "\n",
    "        current_batch_size = len(encoder_input_ids)\n",
    "        predicted_grouped = []\n",
    "        for i in range(current_batch_size):\n",
    "            predicted_grouped.append(predicted_text[i*num_predictions: (i+1)*num_predictions])\n",
    "\n",
    "        for i in range(current_batch_size):\n",
    "            target = target_text[i]\n",
    "            predicted = set(predicted_grouped[i])\n",
    "#             print(target, predxicted)\n",
    "            if target in predicted:\n",
    "                correct += 1\n",
    "            \n",
    "#         if steps % 100 == 0 and steps != 0:\n",
    "#             print(correct/steps)\n",
    "    accuracy = correct/len(dataset)\n",
    "    return accuracy    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b91c8ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                             | 0/5133 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beams: 11, Predictions: 10, Length Penalty: 0.300000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/admin/t5-experiments-from-yura/venv/lib/python3.8/site-packages/pymongo/topology.py:177: UserWarning: MongoClient opened before fork. Create MongoClient only after forking. See PyMongo's documentation for details: https://pymongo.readthedocs.io/en/stable/faq.html#is-pymongo-fork-safe\n",
      "  warnings.warn(\n",
      "/data/home/admin/t5-experiments-from-yura/venv/lib/python3.8/site-packages/transformers/generation_utils.py:2169: UserWarning: `max_length` is deprecated in this function, use `stopping_criteria=StoppingCriteriaList(MaxLengthCriteria(max_length=max_length))` instead.\n",
      "  warnings.warn(\n",
      " 50%|█████████████████▏                | 2592/5133 [07:56<07:46,  5.44batches/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 10.92 GiB total capacity; 9.42 GiB already allocated; 9.56 MiB free; 10.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43meval_multi_old\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(accuracy)\n",
      "Cell \u001b[0;32mIn [9], line 38\u001b[0m, in \u001b[0;36meval_multi_old\u001b[0;34m(model, dataset, args)\u001b[0m\n\u001b[1;32m     30\u001b[0m beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m     31\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(encoder_input_ids),\n\u001b[1;32m     32\u001b[0m     num_beams\u001b[38;5;241m=\u001b[39mbeam_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m     length_penalty \u001b[38;5;241m=\u001b[39m length_penalty\n\u001b[1;32m     36\u001b[0m )\n\u001b[1;32m     37\u001b[0m logits_processor \u001b[38;5;241m=\u001b[39m LogitsProcessorList([])\n\u001b[0;32m---> 38\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeam_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam_scorer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# outputs = model.generate(input_ids = encoder_input_ids)\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# target_text = dataset.tokenizer.batch_decode(labels, skip_special_tokens=True)\u001b[39;00m\n\u001b[1;32m     41\u001b[0m predicted_text \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(outputs, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/data/home/admin/t5-experiments-from-yura/venv/lib/python3.8/site-packages/transformers/generation_utils.py:2305\u001b[0m, in \u001b[0;36mGenerationMixin.beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   2301\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   2302\u001b[0m     outputs, model_kwargs, is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder\n\u001b[1;32m   2303\u001b[0m )\n\u001b[1;32m   2304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2305\u001b[0m     model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reorder_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpast\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2307\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict_in_generate \u001b[38;5;129;01mand\u001b[39;00m output_scores:\n\u001b[1;32m   2308\u001b[0m     beam_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m((beam_indices[beam_idx[i]] \u001b[38;5;241m+\u001b[39m (beam_idx[i],) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(beam_indices))))\n",
      "File \u001b[0;32m/data/home/admin/t5-experiments-from-yura/venv/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py:1746\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration._reorder_cache\u001b[0;34m(self, past, beam_idx)\u001b[0m\n\u001b[1;32m   1742\u001b[0m reordered_layer_past_states \u001b[38;5;241m=\u001b[39m ()\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_past_state \u001b[38;5;129;01min\u001b[39;00m layer_past_states:\n\u001b[1;32m   1744\u001b[0m     \u001b[38;5;66;03m# need to set correct `past` for each of the four key / value states\u001b[39;00m\n\u001b[1;32m   1745\u001b[0m     reordered_layer_past_states \u001b[38;5;241m=\u001b[39m reordered_layer_past_states \u001b[38;5;241m+\u001b[39m (\n\u001b[0;32m-> 1746\u001b[0m         \u001b[43mlayer_past_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_select\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam_idx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer_past_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1747\u001b[0m     )\n\u001b[1;32m   1749\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m reordered_layer_past_states[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m layer_past_states[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(reordered_layer_past_states) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(layer_past_states)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 10.92 GiB total capacity; 9.42 GiB already allocated; 9.56 MiB free; 10.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "accuracy = eval_multi_old(model, dataset, args)\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c736a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15c0bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset, batch_size=16, shuffle=False, num_workers=2,\n",
    "                        collate_fn=dataset._collate_eval)\n",
    "loader = tqdm(data_loader, total=len(data_loader), unit=\"batches\")\n",
    "\n",
    "for steps, batch in enumerate(loader):\n",
    "    encoder_input_ids, attention_mask, target_text = batch\n",
    "    print(encoder_input_ids)\n",
    "    input_ = dataset.tokenizer.batch_decode(encoder_input_ids, skip_special_tokens=True)\n",
    "    output = dataset.tokenizer.batch_decode(model.generate(encoder_input_ids.to('cuda')), skip_special_tokens=True)\n",
    "    print(input_, output, target_text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f70af2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ea3497",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
